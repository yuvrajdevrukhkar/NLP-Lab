{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdB2fq8dnjJyUkYBpYkFt7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuvrajdevrukhkar/NLP-Lab/blob/main/NLP1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrRgRIQOx_62",
        "outputId": "e8db5f5d-4cf1-49ab-d769-fdc7a1bb902a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Whitespace Tokenizer:\n",
            "['NLTK', 'is', 'a', 'powerful', 'Python', 'library', 'for', 'natural', 'language', 'processing', 'tasks', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources,', 'such', 'as', 'WordNet,', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification,', 'tokenization,', 'stemming,', 'tagging,', 'parsing,', 'and', 'semantic', 'reasoning,', 'wrappers', 'for', 'industrial-strength', 'NLP', 'libraries,', 'and', 'an', 'active', 'discussion', 'forum.']\n",
            "\n",
            "Punctuation-based Tokenizer:\n",
            "['NLTK', 'is', 'a', 'powerful', 'Python', 'library', 'for', 'natural', 'language', 'processing', 'tasks', 'It', 'provides', 'easy', '-', 'to', '-', 'use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', ',', 'such', 'as', 'WordNet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', ',', 'wrappers', 'for', 'industrial', '-', 'strength', 'NLP', 'libraries', ',', 'and', 'an', 'active', 'discussion', 'forum', '.']\n",
            "\n",
            "Treebank Tokenizer:\n",
            "['NLTK', 'is', 'a', 'powerful', 'Python', 'library', 'for', 'natural', 'language', 'processing', 'tasks', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', ',', 'such', 'as', 'WordNet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', ',', 'wrappers', 'for', 'industrial-strength', 'NLP', 'libraries', ',', 'and', 'an', 'active', 'discussion', 'forum', '.']\n",
            "\n",
            "Tweet Tokenizer:\n",
            "['NLTK', 'is', 'a', 'powerful', 'Python', 'library', 'for', 'natural', 'language', 'processing', 'tasks', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', ',', 'such', 'as', 'WordNet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', ',', 'wrappers', 'for', 'industrial-strength', 'NLP', 'libraries', ',', 'and', 'an', 'active', 'discussion', 'forum', '.']\n",
            "\n",
            "MWE Tokenizer:\n",
            "['N', 'L', 'T', 'K', ' ', 'i', 's', ' ', 'a', ' ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 'P', 'y', 't', 'h', 'o', 'n', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'y', ' ', 'f', 'o', 'r', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', 't', 'a', 's', 'k', 's', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', 'I', 't', ' ', 'p', 'r', 'o', 'v', 'i', 'd', 'e', 's', ' ', 'e', 'a', 's', 'y', '-', 't', 'o', '-', 'u', 's', 'e', ' ', 'i', 'n', 't', 'e', 'r', 'f', 'a', 'c', 'e', 's', ' ', 't', 'o', ' ', 'o', 'v', 'e', 'r', ' ', '5', '0', ' ', 'c', 'o', 'r', 'p', 'o', 'r', 'a', ' ', 'a', 'n', 'd', ' ', 'l', 'e', 'x', 'i', 'c', 'a', 'l', ' ', 'r', 'e', 's', 'o', 'u', 'r', 'c', 'e', 's', ',', ' ', 's', 'u', 'c', 'h', ' ', 'a', 's', ' ', 'W', 'o', 'r', 'd', 'N', 'e', 't', ',', ' ', 'a', 'l', 'o', 'n', 'g', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 's', 'u', 'i', 't', 'e', ' ', 'o', 'f', ' ', 't', 'e', 'x', 't', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'i', 'e', 's', ' ', '\\n', 'f', 'o', 'r', ' ', 'c', 'l', 'a', 's', 's', 'i', 'f', 'i', 'c', 'a', 't', 'i', 'o', 'n', ',', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ',', ' ', 's', 't', 'e', 'm', 'm', 'i', 'n', 'g', ',', ' ', 't', 'a', 'g', 'g', 'i', 'n', 'g', ',', ' ', 'p', 'a', 'r', 's', 'i', 'n', 'g', ',', ' ', '\\n', 'a', 'n', 'd', ' ', 's', 'e', 'm', 'a', 'n', 't', 'i', 'c', ' ', 'r', 'e', 'a', 's', 'o', 'n', 'i', 'n', 'g', ',', ' ', 'w', 'r', 'a', 'p', 'p', 'e', 'r', 's', ' ', 'f', 'o', 'r', ' ', 'i', 'n', 'd', 'u', 's', 't', 'r', 'i', 'a', 'l', '-', 's', 't', 'r', 'e', 'n', 'g', 't', 'h', ' ', 'N', 'L', 'P', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'i', 'e', 's', ',', ' ', '\\n', 'a', 'n', 'd', ' ', 'a', 'n', ' ', 'a', 'c', 't', 'i', 'v', 'e', ' ', 'd', 'i', 's', 'c', 'u', 's', 's', 'i', 'o', 'n', ' ', 'f', 'o', 'r', 'u', 'm', '.']\n",
            "\n",
            "Porter Stemmer:\n",
            "['n', 'l', 't', 'k', ' ', 'i', 's', ' ', 'a', ' ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 'p', 'y', 't', 'h', 'o', 'n', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'y', ' ', 'f', 'o', 'r', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', 't', 'a', 's', 'k', 's', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', 'i', 't', ' ', 'p', 'r', 'o', 'v', 'i', 'd', 'e', 's', ' ', 'e', 'a', 's', 'y', '-', 't', 'o', '-', 'u', 's', 'e', ' ', 'i', 'n', 't', 'e', 'r', 'f', 'a', 'c', 'e', 's', ' ', 't', 'o', ' ', 'o', 'v', 'e', 'r', ' ', '5', '0', ' ', 'c', 'o', 'r', 'p', 'o', 'r', 'a', ' ', 'a', 'n', 'd', ' ', 'l', 'e', 'x', 'i', 'c', 'a', 'l', ' ', 'r', 'e', 's', 'o', 'u', 'r', 'c', 'e', 's', ',', ' ', 's', 'u', 'c', 'h', ' ', 'a', 's', ' ', 'w', 'o', 'r', 'd', 'n', 'e', 't', ',', ' ', 'a', 'l', 'o', 'n', 'g', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 's', 'u', 'i', 't', 'e', ' ', 'o', 'f', ' ', 't', 'e', 'x', 't', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'i', 'e', 's', ' ', '\\n', 'f', 'o', 'r', ' ', 'c', 'l', 'a', 's', 's', 'i', 'f', 'i', 'c', 'a', 't', 'i', 'o', 'n', ',', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ',', ' ', 's', 't', 'e', 'm', 'm', 'i', 'n', 'g', ',', ' ', 't', 'a', 'g', 'g', 'i', 'n', 'g', ',', ' ', 'p', 'a', 'r', 's', 'i', 'n', 'g', ',', ' ', '\\n', 'a', 'n', 'd', ' ', 's', 'e', 'm', 'a', 'n', 't', 'i', 'c', ' ', 'r', 'e', 'a', 's', 'o', 'n', 'i', 'n', 'g', ',', ' ', 'w', 'r', 'a', 'p', 'p', 'e', 'r', 's', ' ', 'f', 'o', 'r', ' ', 'i', 'n', 'd', 'u', 's', 't', 'r', 'i', 'a', 'l', '-', 's', 't', 'r', 'e', 'n', 'g', 't', 'h', ' ', 'n', 'l', 'p', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'i', 'e', 's', ',', ' ', '\\n', 'a', 'n', 'd', ' ', 'a', 'n', ' ', 'a', 'c', 't', 'i', 'v', 'e', ' ', 'd', 'i', 's', 'c', 'u', 's', 's', 'i', 'o', 'n', ' ', 'f', 'o', 'r', 'u', 'm', '.']\n",
            "\n",
            "Snowball Stemmer:\n",
            "['n', 'l', 't', 'k', ' ', 'i', 's', ' ', 'a', ' ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 'p', 'y', 't', 'h', 'o', 'n', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'y', ' ', 'f', 'o', 'r', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', 't', 'a', 's', 'k', 's', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', 'i', 't', ' ', 'p', 'r', 'o', 'v', 'i', 'd', 'e', 's', ' ', 'e', 'a', 's', 'y', '-', 't', 'o', '-', 'u', 's', 'e', ' ', 'i', 'n', 't', 'e', 'r', 'f', 'a', 'c', 'e', 's', ' ', 't', 'o', ' ', 'o', 'v', 'e', 'r', ' ', '5', '0', ' ', 'c', 'o', 'r', 'p', 'o', 'r', 'a', ' ', 'a', 'n', 'd', ' ', 'l', 'e', 'x', 'i', 'c', 'a', 'l', ' ', 'r', 'e', 's', 'o', 'u', 'r', 'c', 'e', 's', ',', ' ', 's', 'u', 'c', 'h', ' ', 'a', 's', ' ', 'w', 'o', 'r', 'd', 'n', 'e', 't', ',', ' ', 'a', 'l', 'o', 'n', 'g', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 's', 'u', 'i', 't', 'e', ' ', 'o', 'f', ' ', 't', 'e', 'x', 't', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'i', 'e', 's', ' ', '\\n', 'f', 'o', 'r', ' ', 'c', 'l', 'a', 's', 's', 'i', 'f', 'i', 'c', 'a', 't', 'i', 'o', 'n', ',', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ',', ' ', 's', 't', 'e', 'm', 'm', 'i', 'n', 'g', ',', ' ', 't', 'a', 'g', 'g', 'i', 'n', 'g', ',', ' ', 'p', 'a', 'r', 's', 'i', 'n', 'g', ',', ' ', '\\n', 'a', 'n', 'd', ' ', 's', 'e', 'm', 'a', 'n', 't', 'i', 'c', ' ', 'r', 'e', 'a', 's', 'o', 'n', 'i', 'n', 'g', ',', ' ', 'w', 'r', 'a', 'p', 'p', 'e', 'r', 's', ' ', 'f', 'o', 'r', ' ', 'i', 'n', 'd', 'u', 's', 't', 'r', 'i', 'a', 'l', '-', 's', 't', 'r', 'e', 'n', 'g', 't', 'h', ' ', 'n', 'l', 'p', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'i', 'e', 's', ',', ' ', '\\n', 'a', 'n', 'd', ' ', 'a', 'n', ' ', 'a', 'c', 't', 'i', 'v', 'e', ' ', 'd', 'i', 's', 'c', 'u', 's', 's', 'i', 'o', 'n', ' ', 'f', 'o', 'r', 'u', 'm', '.']\n",
            "\n",
            "Lemmatization:\n",
            "['N', 'L', 'T', 'K', ' ', 'i', 's', ' ', 'a', ' ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 'P', 'y', 't', 'h', 'o', 'n', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'y', ' ', 'f', 'o', 'r', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', 't', 'a', 's', 'k', 's', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', 'I', 't', ' ', 'p', 'r', 'o', 'v', 'i', 'd', 'e', 's', ' ', 'e', 'a', 's', 'y', '-', 't', 'o', '-', 'u', 's', 'e', ' ', 'i', 'n', 't', 'e', 'r', 'f', 'a', 'c', 'e', 's', ' ', 't', 'o', ' ', 'o', 'v', 'e', 'r', ' ', '5', '0', ' ', 'c', 'o', 'r', 'p', 'o', 'r', 'a', ' ', 'a', 'n', 'd', ' ', 'l', 'e', 'x', 'i', 'c', 'a', 'l', ' ', 'r', 'e', 's', 'o', 'u', 'r', 'c', 'e', 's', ',', ' ', 's', 'u', 'c', 'h', ' ', 'a', 's', ' ', 'W', 'o', 'r', 'd', 'N', 'e', 't', ',', ' ', 'a', 'l', 'o', 'n', 'g', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 's', 'u', 'i', 't', 'e', ' ', 'o', 'f', ' ', 't', 'e', 'x', 't', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'i', 'e', 's', ' ', '\\n', 'f', 'o', 'r', ' ', 'c', 'l', 'a', 's', 's', 'i', 'f', 'i', 'c', 'a', 't', 'i', 'o', 'n', ',', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ',', ' ', 's', 't', 'e', 'm', 'm', 'i', 'n', 'g', ',', ' ', 't', 'a', 'g', 'g', 'i', 'n', 'g', ',', ' ', 'p', 'a', 'r', 's', 'i', 'n', 'g', ',', ' ', '\\n', 'a', 'n', 'd', ' ', 's', 'e', 'm', 'a', 'n', 't', 'i', 'c', ' ', 'r', 'e', 'a', 's', 'o', 'n', 'i', 'n', 'g', ',', ' ', 'w', 'r', 'a', 'p', 'p', 'e', 'r', 's', ' ', 'f', 'o', 'r', ' ', 'i', 'n', 'd', 'u', 's', 't', 'r', 'i', 'a', 'l', '-', 's', 't', 'r', 'e', 'n', 'g', 't', 'h', ' ', 'N', 'L', 'P', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'i', 'e', 's', ',', ' ', '\\n', 'a', 'n', 'd', ' ', 'a', 'n', ' ', 'a', 'c', 't', 'i', 'v', 'e', ' ', 'd', 'i', 's', 'c', 'u', 's', 's', 'i', 'o', 'n', ' ', 'f', 'o', 'r', 'u', 'm', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer, TweetTokenizer, MWETokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample text\n",
        "text = '''NLTK is a powerful Python library for natural language processing tasks\n",
        "      It provides easy-to-use interfaces to over 50 corpora and lexical resources, such as WordNet, along with a suite of text processing libraries\n",
        "for classification, tokenization, stemming, tagging, parsing,\n",
        "and semantic reasoning, wrappers for industrial-strength NLP libraries,\n",
        "and an active discussion forum.'''\n",
        "\n",
        "# Tokenization methods\n",
        "tokenizers = {\n",
        "    \"Whitespace Tokenizer\": WhitespaceTokenizer(),\n",
        "    \"Punctuation-based Tokenizer\": WordPunctTokenizer(),\n",
        "    \"Treebank Tokenizer\": TreebankWordTokenizer(),\n",
        "    \"Tweet Tokenizer\": TweetTokenizer(),\n",
        "    \"MWE Tokenizer\": MWETokenizer(),\n",
        "}\n",
        "\n",
        "# Stemmers\n",
        "porter_stemmer = PorterStemmer()\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenization\n",
        "for name, tokenizer in tokenizers.items():\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(tokens)\n",
        "\n",
        "# Stemming\n",
        "print(\"\\nPorter Stemmer:\")\n",
        "porter_stems = [porter_stemmer.stem(token) for token in tokens]\n",
        "print(porter_stems)\n",
        "\n",
        "print(\"\\nSnowball Stemmer:\")\n",
        "snowball_stems = [snowball_stemmer.stem(token) for token in tokens]\n",
        "print(snowball_stems)\n",
        "\n",
        "# Lemmatization\n",
        "print(\"\\nLemmatization:\")\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(lemmas)\n"
      ]
    }
  ]
}